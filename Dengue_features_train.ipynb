{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adjusted-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, plot_roc_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "url = 'http://localhost:8888/edit/Dengue_features_train/dengue_labels_train.csv'\n",
    "dengue_labels_train = pd.read_csv('Dengue_features_train/dengue_labels_train.csv')\n",
    "dengue_features_train = pd.read_csv('Dengue_features_train/dengue_features_train.csv')\n",
    "dengue_features_test = pd.read_csv('Dengue_features_train/dengue_features_test (1).csv')\n",
    "submission_format = pd.read_csv('Dengue_features_train/submission_format (4).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-shape",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "raw",
   "id": "offshore-guidance",
   "metadata": {},
   "source": [
    "Dengue fever is a mosquito-borne disease that occurs in tropical and sub-tropical parts of the world. In mild cases, symptoms are similar to the flu: fever, rash, and muscle and joint pain. In severe cases, dengue fever can cause severe bleeding, low blood pressure, and even death.\n",
    "\n",
    "Because it is carried by mosquitoes, the transmission dynamics of dengue are related to climate variables such as temperature and precipitation. Although the relationship to climate is complex, a growing number of scientists argue that climate change is likely to produce distributional shifts that will have significant public health implications worldwide.\n",
    "\n",
    "In recent years dengue fever has been spreading. Historically, the disease has been most prevalent in Southeast Asia and the Pacific islands. These days many of the nearly half billion cases per year are occurring in Latin America."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-stupid",
   "metadata": {},
   "source": [
    "# Research Background"
   ]
  },
  {
   "cell_type": "raw",
   "id": "right-exhibition",
   "metadata": {},
   "source": [
    "The data comes from multiple sources aimed at supporting the Predict the Next Pandemic Initiative. Dengue surveillance data is provided by the U.S. Centers for Disease Control and prevention, as well as the Department of Defense's Naval Medical Research Unit 6 and the Armed Forces Health Surveillance Center, in collaboration with the Peruvian government and U.S. universities. Environmental and climate data is provided by the National Oceanic and Atmospheric Administration (NOAA), an agency of the U.S. Department of Commerce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-trainer",
   "metadata": {},
   "source": [
    "# Problem Statement(Prediction)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bearing-clause",
   "metadata": {},
   "source": [
    "The problem statement (goal) is to predict the total_cases label for each (city, year, week of the year) in the test set. There are two cities, San Juan and Iquitos, with test data for each city spanning 5 and 3 years respectively. This will make one submission that contains predictions for both cities. The data for each city have been concatenated along with a city column indicating the source: sj for San Juan and iq for Iquitos. The test set is a pure future hold-out, meaning the test data are sequential and non-overlapping with any of the training data. Throughout, missing values have been filled as NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-glance",
   "metadata": {},
   "source": [
    "# Features and Observations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sublime-electric",
   "metadata": {},
   "source": [
    "Obeservation and Features of datsets\n",
    "List of features\n",
    "Performance metric\n",
    "Mean absolute error\n",
    "Submission Format Format\n",
    "The features in this dataset City and date indicators city –\n",
    "City abbreviations:\n",
    "sj for San Juan and iq for Iquitos\n",
    "week_start_date –Date given in yyyy-mm-dd format\n",
    "NOAA's GHCN daily climate data weather station measurements station_\n",
    "max_temp_c – Maximum temperature station_\n",
    "min_temp_c – Minimum temperature station_\n",
    "avg_temp_c – Average temperature station_\n",
    "precip_mm – Total precipitation station_\n",
    "diur_temp_rng_c – Diurnal temperature range PERSIANN satellite precipitation measurements (0.25x0.25 degree scale)\n",
    "precipitation_amt_mm – Total precipitation NOAA's NCEP Climate Forecast System Reanalysis measurements (0.5x0.5 degree scale) reanalysis\n",
    "sat_precip_amt_mm –Total precipitation reanalysis_\n",
    "dew_point_temp_k – Mean dew point temperature reanalysis_\n",
    "air_temp_k – Mean air temperature reanalysis_\n",
    "relative_humidity_percent – Mean relative humidity reanalysis_specific_humidity_\n",
    "g_per_kg – Mean specific humidity reanalysis_\n",
    "precip_amt_kg_per_m2 – Total precipitation reanalysis_\n",
    "max_air_temp_k – Maximum air temperature reanalysis_\n",
    "min_air_temp_k –Minimum air temperature reanalysis_\n",
    "avg_temp_k – Average air temperature reanalysis_\n",
    "tdtr_k –Diurnal temperature range Satellite vegetation -\n",
    "Normalized difference vegetation index (NDVI) -\n",
    "NOAA's CDR Normalized Difference Vegetation Index (0.5x0.5 degree scale) measurements\n",
    "ndvi_se – Pixel southeast of city centroid\n",
    "ndvi_sw – Pixel southwest of city centroid\n",
    "ndvi_ne –Pixel northeast of city centroid\n",
    "ndvi_nw – Pixel northwest of city centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-squad",
   "metadata": {},
   "source": [
    "# Summary of the Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "minor-pursuit",
   "metadata": {},
   "source": [
    "The current epidemics of dengue fever diseases have been a burning issue. Hence there is need to resolve The statical significant and relationship between climate and dengue dynamics can improve research initiatives and resource allocation to help fight life-threatening pandemics.\n",
    " Hence we will be able to predict the number of dengue cases each week (in each location) based on environmental variables describing changes in temperature, precipitation, vegetation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-bandwidth",
   "metadata": {},
   "source": [
    "### METHODOLOGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ordinary-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OrdinalEncoder, OneHotEncoder \n",
    "#import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression,Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from numpy.random import permutation\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thermal-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'http://localhost:8888/edit/Dengue_features_train/dengue_labels_train.csv'\n",
    "dengue_labels_train = pd.read_csv('Dengue_features_train/dengue_labels_train.csv')\n",
    "dengue_features_train = pd.read_csv('Dengue_features_train/dengue_features_train.csv')\n",
    "dengue_features_test = pd.read_csv('Dengue_features_train/dengue_features_test (1).csv')\n",
    "submission_format = pd.read_csv('Dengue_features_train/submission_format (4).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "potential-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_join = pd.merge(dengue_labels_train,\n",
    "         dengue_features_train,\n",
    "         how= 'right', on=['city','year', 'weekofyear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-crown",
   "metadata": {},
   "source": [
    "### 1. WRANGLE THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "informal-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle():  \n",
    "      df = pd.merge(dengue_labels_train,\n",
    "         dengue_features_train,\n",
    "         how= 'right', on=['city','year', 'weekofyear'])\n",
    "\n",
    "    # Drop constant and repeated columns\n",
    "      df.drop(columns=['ndvi_ne',\t'ndvi_nw','ndvi_se'], \n",
    "          inplace=True)\n",
    "  \n",
    "    # Drop columns with high % of NaN values\n",
    "      df.dropna(axis=1, thresh=len(df)*.6, inplace=True)\n",
    "      df.fillna(0, inplace=True)\n",
    "\n",
    "      return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "demonstrated-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= wrangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "primary-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = wrangle(dengue_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dengue_labels_train.shape\n",
    "#dengue_features_train.shape\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-pizza",
   "metadata": {},
   "source": [
    "### SPLIT THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aquatic-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our feature matrix and target vector\n",
    "target = 'total_cases'\n",
    "y = df[target]\n",
    "X = df.drop(columns=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_test, y_test = train_test_split(dengue_features_test, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "featured-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train -valid split Train before the year 2010 and test in the year2010\n",
    "mask1 = X.index <= 2009\n",
    "mask2 = X.index == 2010\n",
    "X_train, y_train = X.loc[mask1], y.loc[mask1]\n",
    "X_val, y_val = X.loc[mask2], y.loc[mask2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-reporter",
   "metadata": {},
   "source": [
    "### ESTABLISH A BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "surgical-motivation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_total_cases: 24.67513736263736\n",
      "Baseline MAE: 23.003354818258664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print('mean_total_cases:', y_train.mean())\n",
    "y_pred = [y_train.mean()] * len(y_train)\n",
    "print('Baseline MAE:', mean_absolute_error(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "useful-georgia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline mae is 23.003354818258664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "baseline_mae = mean_absolute_error(y_train, [y_train.mean()]*len(y_train))\n",
    "print('baseline mae is', baseline_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-richmond",
   "metadata": {},
   "source": [
    "### BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "racial-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['city', 'week_start_date'],\n",
       "                               use_cat_names=True)),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from category_encoders import OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model_lr = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
    "                         LinearRegression())\n",
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "blocked-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city_sj                       1.544472e+05\n",
       "city_iq                       1.544299e+05\n",
       "year                         -1.884208e+08\n",
       "weekofyear                   -2.619227e+07\n",
       "week_start_date_1990-04-30   -2.498907e+09\n",
       "                                  ...     \n",
       "station_avg_temp_c            2.068443e-01\n",
       "station_diur_temp_rng_c      -1.198756e+00\n",
       "station_max_temp_c            5.921605e-01\n",
       "station_min_temp_c            2.329604e-01\n",
       "station_precip_mm            -8.886814e-03\n",
       "Length: 1070, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = model_lr.named_steps['linearregression'].coef_\n",
    "features = model_lr.named_steps['onehotencoder'].get_feature_names()\n",
    "pd.Series(coefficients, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deluxe-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder', OrdinalEncoder(cols=[], mapping=[])),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('gradientboostingregressor',\n",
       "                 GradientBoostingRegressor(random_state=42))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Module2\n",
    "#FIT GRADIENT BOOSTING MODEL\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "oe = OrdinalEncoder()\n",
    "X_train_T = oe.fit_transform(X_train)\n",
    "X_val_T = oe.transform(X_val)\n",
    "X_test = oe.transform(X_test)\n",
    "model_gb =  make_pipeline(OrdinalEncoder(),\n",
    "                         SimpleImputer(),\n",
    "                         GradientBoostingRegressor(random_state=42))\n",
    "\n",
    "model_gb.fit(X_train_T, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "about-registrar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder', OrdinalEncoder(cols=[], mapping=[])),\n",
       "                ('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=10, num_parallel_tree=1, random_state=42,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Module 3 FIT XGRADIENT BOOSTING MODEL\n",
    "from xgboost import XGBRegressor\n",
    "oe = OrdinalEncoder()\n",
    "X_train_T = oe.fit_transform(X_train)\n",
    "X_val_T = oe.transform(X_val)\n",
    "model_xg =  make_pipeline(OrdinalEncoder(),\n",
    "                         SimpleImputer(strategy='median'),\n",
    "                         XGBRegressor(random_state=42, n_jobs=10))\n",
    "\n",
    "model_xg.fit(X_train_T, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "federal-timing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder',\n",
       "                 OrdinalEncoder(cols=['city', 'week_start_date'],\n",
       "                                mapping=[{'col': 'city',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': sj     1\n",
       "iq     2\n",
       "NaN   -2\n",
       "dtype: int64},\n",
       "                                         {'col': 'week_start_date',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': 1990-04-30       1\n",
       "1990-05-07       2\n",
       "1990-05-14       3\n",
       "1990-05-21       4\n",
       "1990-05-28       5\n",
       "              ... \n",
       "2010-06-04    1046\n",
       "2010-06-11    1047\n",
       "2010-06-18    1048\n",
       "2010-06-25    1049\n",
       "NaN             -2\n",
       "Length: 1050, dtype: int64}])),\n",
       "                ('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('randomforestregressor',\n",
       "                 RandomForestRegressor(random_state=42))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = make_pipeline(OrdinalEncoder(),\n",
    "                        SimpleImputer(strategy='median'),\n",
    "                        RandomForestRegressor(random_state=42))\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "olive-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder',\n",
       "                 OrdinalEncoder(cols=['city', 'week_start_date'],\n",
       "                                mapping=[{'col': 'city',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': sj     1\n",
       "iq     2\n",
       "NaN   -2\n",
       "dtype: int64},\n",
       "                                         {'col': 'week_start_date',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': 1990-04-30       1\n",
       "1990-05-07       2\n",
       "1990-05-14       3\n",
       "1990-05-21       4\n",
       "1990-05-28       5\n",
       "              ... \n",
       "2010-06-04    1046\n",
       "2010-06-11    1047\n",
       "2010-06-18    1048\n",
       "2010-06-25    1049\n",
       "NaN             -2\n",
       "Length: 1050, dtype: int64}])),\n",
       "                ('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                ('decisiontreeregressor',\n",
       "                 DecisionTreeRegressor(random_state=42))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dt = make_pipeline(OrdinalEncoder(),\n",
    "                        SimpleImputer(strategy='median'),\n",
    "                        DecisionTreeRegressor(random_state=42))\n",
    "model_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "authorized-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "oe = OrdinalEncoder()\n",
    "oe.fit(X_train)\n",
    "XT_train = oe.transform(X_train)\n",
    "XT_test  = oe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "boxed-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = make_pipeline(\n",
    "          OrdinalEncoder(),\n",
    "          SimpleImputer(),\n",
    "          LinearRegression()\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "understood-irish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('ordinalencoder',\n",
       "                 OrdinalEncoder(cols=['city', 'week_start_date'],\n",
       "                                mapping=[{'col': 'city',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': sj     1\n",
       "iq     2\n",
       "NaN   -2\n",
       "dtype: int64},\n",
       "                                         {'col': 'week_start_date',\n",
       "                                          'data_type': dtype('O'),\n",
       "                                          'mapping': 1990-04-30       1\n",
       "1990-05-07       2\n",
       "1990-05-14       3\n",
       "1990-05-21       4\n",
       "1990-05-28       5\n",
       "              ... \n",
       "2010-06-04    1046\n",
       "2010-06-11    1047\n",
       "2010-06-18    1048\n",
       "2010-06-25    1049\n",
       "NaN             -2\n",
       "Length: 1050, dtype: int64}])),\n",
       "                ('simpleimputer', SimpleImputer()), ('ridge', Ridge())])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model_r =make_pipeline(\n",
    "          OrdinalEncoder(),\n",
    "          SimpleImputer(),\n",
    "          Ridge()\n",
    ")\n",
    "model_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "hawaiian-circulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_lr = make_pipeline(\n",
    "          OrdinalEncoder(),\n",
    "          SimpleImputer(),\n",
    "          LogisticRegression()\n",
    ")\n",
    "model_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-layout",
   "metadata": {},
   "source": [
    "### CHECK THE METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "vanilla-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 20.92588631597003\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-82f45acae815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training MAE:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Validity MAE:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m         \u001b[0mstatistics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatistics_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[0m_check_inputs_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\impute\\_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             X = self._validate_data(X, reset=in_fit,\n\u001b[0m\u001b[0;32m    253\u001b[0m                                     \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                                     \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\israe\\.virtualenvs\\github-mpyjtti5\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m             raise ValueError(\"Found array with %d sample(s) (shape=%s) while a\"\n\u001b[0m\u001b[0;32m    670\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "#Mean_Absolute_Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('Training MAE:', mean_absolute_error(y_train, model.predict(X_train)))\n",
    "print('Validity MAE:', mean_absolute_error(y_val, model.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-dinner",
   "metadata": {},
   "source": [
    "#Root_Mean_Squared_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('Training MAE:', mean_squared_error(y_train, model.predict(X_train), squared=False))\n",
    "print('Test MAE:', mean_squared_error(y_test, model.predict(X_test), squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train,model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-germany",
   "metadata": {},
   "source": [
    "### TUNE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-buffer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-australia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "numeric-seattle",
   "metadata": {},
   "source": [
    "### RESULTS OF THE FINIDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "feature_names = ohe.get_feature_names()\n",
    "coefficients = model_lr.coef_\n",
    "feature_importances = pd.Series(coefficients, index=feature_names).sort_values(key=abs)\n",
    "feature_importances.tail(10).plot(kind='barh', title='Linear Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ohe.get_feature_names()\n",
    "coefficients = model_r.coef_\n",
    "feature_importances = pd.Series(coefficients, index=feature_names).sort_values(key=abs)\n",
    "feature_importances.tail(10).plot(kind='barh', title='Ridge Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-navigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "crazy-sacramento",
   "metadata": {},
   "source": [
    "#Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_xg.named_steps['xgbclassifier'].feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "feat_imp = pd.Series(importances, index=features).sort_values()\n",
    "feat_imp.tail(10).plot(kind='barh')\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for model_xg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-escape",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
